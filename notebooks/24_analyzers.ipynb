{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "To read more about analyzers, checkout the docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html).\n",
    "\n",
    "![analyzer_api](../images/analyzer_api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'DlYG5m9gR3upn7qgaYyAJA',\n",
      " 'name': '3d37442d2591',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-08-05T10:05:34.233336849Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '1a77947f34deddb41af25e6f0ddb8e830159c179',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.11.1',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.15.0'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "client_info = es.info()\n",
    "print('Connected to Elasticsearch!')\n",
    "pprint(client_info.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Character filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about them [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. HTML Strip Character Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 26,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': \"I'm so happy!\\n\",\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = es.indices.analyze(\n",
    "    char_filter=[\n",
    "        \"html_strip\"\n",
    "    ],\n",
    "    text=\"I&apos;m so happy</b>!</p>\",\n",
    ")\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Mapping character filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 37,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': 'I saw comet Tsuchinshan Atlas in 2024',\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"keyword\",\n",
    "    char_filter=[\n",
    "        {\n",
    "            \"type\": \"mapping\",\n",
    "            \"mappings\": [\n",
    "                \"٠ => 0\",\n",
    "                \"١ => 1\",\n",
    "                \"٢ => 2\",\n",
    "                \"٣ => 3\",\n",
    "                \"٤ => 4\",\n",
    "                \"٥ => 5\",\n",
    "                \"٦ => 6\",\n",
    "                \"٧ => 7\",\n",
    "                \"٨ => 8\",\n",
    "                \"٩ => 9\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about tokenizers [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The', Type: <ALPHANUM>\n",
      "Token: '2', Type: <NUM>\n",
      "Token: 'QUICK', Type: <ALPHANUM>\n",
      "Token: 'Brown', Type: <ALPHANUM>\n",
      "Token: 'Foxes', Type: <ALPHANUM>\n",
      "Token: 'jumped', Type: <ALPHANUM>\n",
      "Token: 'over', Type: <ALPHANUM>\n",
      "Token: 'the', Type: <ALPHANUM>\n",
      "Token: 'lazy', Type: <ALPHANUM>\n",
      "Token: 'dog's', Type: <ALPHANUM>\n",
      "Token: 'bone', Type: <ALPHANUM>\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'the', Type: word\n",
      "Token: 'quick', Type: word\n",
      "Token: 'brown', Type: word\n",
      "Token: 'foxes', Type: word\n",
      "Token: 'jumped', Type: word\n",
      "Token: 'over', Type: word\n",
      "Token: 'the', Type: word\n",
      "Token: 'lazy', Type: word\n",
      "Token: 'dog', Type: word\n",
      "Token: 's', Type: word\n",
      "Token: 'bone', Type: word\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"lowercase\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The', Type: word\n",
      "Token: '2', Type: word\n",
      "Token: 'QUICK', Type: word\n",
      "Token: 'Brown-Foxes', Type: word\n",
      "Token: 'jumped', Type: word\n",
      "Token: 'over', Type: word\n",
      "Token: 'the', Type: word\n",
      "Token: 'lazy', Type: word\n",
      "Token: 'dog's', Type: word\n",
      "Token: 'bone.', Type: word\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"whitespace\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about token filters [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The'\n",
      "Token: '2'\n",
      "Token: 'QUICK'\n",
      "Token: 'Brown'\n",
      "Token: 'Foxes'\n",
      "Token: 'jumped'\n",
      "Token: 'over'\n",
      "Token: 'the'\n",
      "Token: 'lazy'\n",
      "Token: 'dog'\n",
      "Token: 'bone'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"apostrophe\"\n",
    "    ],\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decimal digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'Tsuchinshan'\n",
      "Token: 'Atlas'\n",
      "Token: 'in'\n",
      "Token: '2024'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"decimal_digit\"\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I'\n",
      "Token: 'was'\n",
      "Token: 'temoc'\n",
      "Token: 'nahsnihcusT'\n",
      "Token: 'saltA'\n",
      "Token: 'ni'\n",
      "Token: '٤٢٠٢'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"reverse\"\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Built-in analyzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about token filters [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n",
      "Token: 'in'\n",
      "Token: '٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"standard\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"stop\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I saw comet Tsuchinshan Atlas in ٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"keyword\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Index time VS Search time analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Index time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index-time analysis transforms text before it's stored in the index. In this example, let's create an index with an analyzer that lowercases text, removes HTML tags, and replaces ampersands (&) with the word \"and.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '1',\n",
      " '_index': 'index_time_example',\n",
      " '_primary_term': 1,\n",
      " '_seq_no': 0,\n",
      " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      " '_version': 1,\n",
      " 'result': 'created'}\n"
     ]
    }
   ],
   "source": [
    "index_name = \"index_time_example\"\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"char_filter\": {\n",
    "                \"ampersand_replacement\": {\n",
    "                    \"type\": \"mapping\",\n",
    "                    \"mappings\": [\"& => and\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_index_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"html_strip\", \"ampersand_replacement\"],\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_index_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es.indices.create(index=index_name, body=settings)\n",
    "\n",
    "document = {\n",
    "    \"content\": \"Visit my website https://myuniversehub.com/ & like some images!\"}\n",
    "response = es.index(index=index_name, id=1, body=document)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching for the document, you'll notice that the content appears unchanged. This is expected because Elasticsearch stores the transformed tokens in an inverted index for searching purposes, while keeping the original document intact in the `_source` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "hits = response.body[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the custom analyzer is working by applying it to the document like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'visit'\n",
      "Token: 'my'\n",
      "Token: 'website'\n",
      "Token: 'https'\n",
      "Token: 'myuniversehub.com'\n",
      "Token: 'and'\n",
      "Token: 'like'\n",
      "Token: 'some'\n",
      "Token: 'images'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"field\": \"content\",\n",
    "        \"text\": \"Visit my website https://myuniversehub.com/ & like some images!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Search time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search-time analysis transforms text only when a search query is performed, not when data is indexed. In this example, we’ll perform a search with a search-time analyzer that transforms text differently (e.g., it lowercases and removes stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"match\": { # match is used for full-text search\n",
    "            \"content\": {\n",
    "                \"query\": \"myuniversehub.com\",\n",
    "                \"analyzer\": \"standard\"  # Using a different analyzer than the one used at index time\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use a `term` query to match exact terms. Since `myuniversehub.com` exists exactly as-is in the document, this query will return the document in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"term\": {  # term is used for exact matches\n",
    "            \"content\": {\n",
    "                \"value\": \"myuniversehub.com\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `MYUNIVERSEHUB.com` does not appear in the document, so no results are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"term\": {  # term is used for exact matches\n",
    "            \"content\": {\n",
    "                \"value\": \"MYUNIVERSEHUB.com\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "To read more about synonyms, checkout the docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/synonyms-apis.html).\n",
    "\n",
    "![synonyms_api_docs](../images/synonyms_api_docs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'iNEgsrfzSs-A5IWMvnKk8w',\n",
      " 'name': '5af1aab6c380',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-08-05T10:05:34.233336849Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '1a77947f34deddb41af25e6f0ddb8e830159c179',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.11.1',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.15.0'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "client_info = es.info()\n",
    "print('Connected to Elasticsearch!')\n",
    "pprint(client_info.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating an index with a custom analyzer that uses a synonym filter to expand terms like \"car\" to include \"automobile\" and \"vehicle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'index': 'my_synonym_index', 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"synonym_filter\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\n",
    "                        \"car, automobile, vehicle\",\n",
    "                        \"tv, television\",\n",
    "                        \"smartphone, mobile, cell phone\",\n",
    "                        \"jupyter, jupyter notebook, jupyterlab\",\n",
    "                        # next one is explicit synonym\n",
    "                        \"jupiter, mars, earth, venus, mercury, saturn, uranus, neptune => planet\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"synonym_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\", # notes: synonyms configurations require lowercase tokens\n",
    "                        \"synonym_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"synonym_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"my_synonym_index\"\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "response = es.indices.create(index=index_name, body=settings)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 76818.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': False,\n",
      " 'items': [{'index': {'_id': 'd0kBNJUB6odEtf1M-oPS',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 5,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'eEkBNJUB6odEtf1M-oPS',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 6,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'eUkBNJUB6odEtf1M-oPS',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 7,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'ekkBNJUB6odEtf1M-oPS',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 8,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'e0kBNJUB6odEtf1M-oPS',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 9,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}}],\n",
      " 'took': 29081070}\n",
      "{'tokens': [{'end_offset': 1,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': 'i',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 6,\n",
      "             'position': 1,\n",
      "             'start_offset': 2,\n",
      "             'token': 'love',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 9,\n",
      "             'position': 2,\n",
      "             'start_offset': 7,\n",
      "             'token': 'my',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 13,\n",
      "             'position': 3,\n",
      "             'start_offset': 10,\n",
      "             'token': 'car',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 13,\n",
      "             'position': 3,\n",
      "             'start_offset': 10,\n",
      "             'token': 'automobile',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 13,\n",
      "             'position': 3,\n",
      "             'start_offset': 10,\n",
      "             'token': 'vehicle',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 17,\n",
      "             'position': 4,\n",
      "             'start_offset': 14,\n",
      "             'token': 'and',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 28,\n",
      "             'position': 5,\n",
      "             'start_offset': 18,\n",
      "             'token': 'television',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 28,\n",
      "             'position': 5,\n",
      "             'start_offset': 18,\n",
      "             'token': 'tv',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 32,\n",
      "             'position': 106,\n",
      "             'start_offset': 30,\n",
      "             'token': 'my',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 43,\n",
      "             'position': 107,\n",
      "             'start_offset': 33,\n",
      "             'token': 'smartphone',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 43,\n",
      "             'position': 107,\n",
      "             'start_offset': 33,\n",
      "             'token': 'mobile',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 43,\n",
      "             'position': 107,\n",
      "             'start_offset': 33,\n",
      "             'token': 'cell',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 46,\n",
      "             'position': 108,\n",
      "             'start_offset': 44,\n",
      "             'token': 'is',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 46,\n",
      "             'position': 108,\n",
      "             'start_offset': 44,\n",
      "             'token': 'phone',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 54,\n",
      "             'position': 109,\n",
      "             'start_offset': 47,\n",
      "             'token': 'amazing',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 57,\n",
      "             'position': 210,\n",
      "             'start_offset': 56,\n",
      "             'token': 'i',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 62,\n",
      "             'position': 211,\n",
      "             'start_offset': 58,\n",
      "             'token': 'love',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 70,\n",
      "             'position': 212,\n",
      "             'start_offset': 63,\n",
      "             'token': 'working',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 75,\n",
      "             'position': 213,\n",
      "             'start_offset': 71,\n",
      "             'token': 'with',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 83,\n",
      "             'position': 214,\n",
      "             'start_offset': 76,\n",
      "             'token': 'jupyter',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 83,\n",
      "             'position': 214,\n",
      "             'start_offset': 76,\n",
      "             'token': 'jupyter',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 83,\n",
      "             'position': 214,\n",
      "             'start_offset': 76,\n",
      "             'token': 'jupyterlab',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 93,\n",
      "             'position': 215,\n",
      "             'start_offset': 84,\n",
      "             'token': 'notebooks',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 93,\n",
      "             'position': 215,\n",
      "             'start_offset': 84,\n",
      "             'token': 'notebook',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 96,\n",
      "             'position': 316,\n",
      "             'start_offset': 95,\n",
      "             'token': 'i',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 101,\n",
      "             'position': 317,\n",
      "             'start_offset': 97,\n",
      "             'token': 'want',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 104,\n",
      "             'position': 318,\n",
      "             'start_offset': 102,\n",
      "             'token': 'to',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 107,\n",
      "             'position': 319,\n",
      "             'start_offset': 105,\n",
      "             'token': 'go',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 110,\n",
      "             'position': 320,\n",
      "             'start_offset': 108,\n",
      "             'token': 'to',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 115,\n",
      "             'position': 321,\n",
      "             'start_offset': 111,\n",
      "             'token': 'planet',\n",
      "             'type': 'SYNONYM'},\n",
      "            {'end_offset': 118,\n",
      "             'position': 422,\n",
      "             'start_offset': 117,\n",
      "             'token': 'i',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 123,\n",
      "             'position': 423,\n",
      "             'start_offset': 119,\n",
      "             'token': 'want',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 126,\n",
      "             'position': 424,\n",
      "             'start_offset': 124,\n",
      "             'token': 'to',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 129,\n",
      "             'position': 425,\n",
      "             'start_offset': 127,\n",
      "             'token': 'go',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 132,\n",
      "             'position': 426,\n",
      "             'start_offset': 130,\n",
      "             'token': 'to',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 138,\n",
      "             'position': 427,\n",
      "             'start_offset': 133,\n",
      "             'token': 'planet',\n",
      "             'type': 'SYNONYM'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm # type: ignore\n",
    "\n",
    "operations = []\n",
    "dummy_data = json.load(open(\"../data/synonyms.json\"))\n",
    "for document in tqdm(dummy_data, total=len(dummy_data)):\n",
    "    operations.append({'index': {'_index': index_name}})\n",
    "    operations.append(document)\n",
    "\n",
    "response = es.bulk(operations=operations)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching with Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s search for terms that should match synonyms. For example, we’ll search for \"vehicle\" (which should match \"car\" or \"automobile\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "{'description': 'I love my car and television.'}\n",
      "{'description': 'I love my car and television.'}\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"description\": \"vehicle\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=query)\n",
    "\n",
    "print(\"Search Results:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "{'description': 'I want to go to Mars.'}\n",
      "{'description': 'I want to go to Venus.'}\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": { # match will also work\n",
    "            \"description\": \"planet\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=query)\n",
    "\n",
    "print(\"Search Results:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Synonyms for Search-Time Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to apply synonyms only during search (and not while indexing), you can modify the search query analyzer like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'index': 'my_synonym_index', 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"synonym_filter\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\n",
    "                        \"car, automobile, vehicle\",\n",
    "                        \"tv, television\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"index_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                },\n",
    "                \"search_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"index_analyzer\",\n",
    "                \"search_analyzer\": \"search_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index=index_name)\n",
    "response = es.indices.create(index=index_name, body=settings)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 30481.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errors': False,\n",
      " 'items': [{'index': {'_id': 'fEkDNJUB6odEtf1Ms4MW',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 0,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'fUkDNJUB6odEtf1Ms4MW',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 1,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'fkkDNJUB6odEtf1Ms4MW',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 2,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'f0kDNJUB6odEtf1Ms4MW',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 3,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}},\n",
      "           {'index': {'_id': 'gEkDNJUB6odEtf1Ms4MW',\n",
      "                      '_index': 'my_synonym_index',\n",
      "                      '_primary_term': 1,\n",
      "                      '_seq_no': 4,\n",
      "                      '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      "                      '_version': 1,\n",
      "                      'result': 'created',\n",
      "                      'status': 201}}],\n",
      " 'took': 29193778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm # type: ignore\n",
    "\n",
    "operations = []\n",
    "dummy_data = json.load(open(\"../data/synonyms.json\"))\n",
    "for document in tqdm(dummy_data, total=len(dummy_data)):\n",
    "    operations.append({'index': {'_index': index_name}})\n",
    "    operations.append(document)\n",
    "\n",
    "response = es.bulk(operations=operations)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results (Search-time synonyms):\n",
      "{'description': 'I love my car and television.'}\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"description\": \"vehicle\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=query)\n",
    "\n",
    "print(\"Search Results (Search-time synonyms):\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
